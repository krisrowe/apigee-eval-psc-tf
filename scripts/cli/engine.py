import os
import sys
import shutil
import hashlib
import json
from pathlib import Path
from rich.console import Console
from scripts.cli.config import Config

console = Console()

class TerraformStager:
    """Stages Terraform configuration in XDG cache for execution."""
    
    CACHE_DIR_NAME = "apim"

    def __init__(self, config: Config):
        self.config = config
        self.project_root = config.root_dir
        self.staging_dir = self._get_staging_dir()
        self.package_root = self._find_package_root()

    def _get_staging_dir(self) -> Path:
        """Calculates stable cache path based on project root hash."""
        xdg_cache = os.environ.get("XDG_CACHE_HOME", str(Path.home() / ".cache"))
        project_hash = hashlib.sha256(str(self.project_root).encode()).hexdigest()[:12]
        return Path(xdg_cache) / self.CACHE_DIR_NAME / "projects" / f"{self.config.project.name}_{project_hash}"

    def _find_package_root(self) -> Path:
        """Locates the installed package root (contains main.tf, modules/, etc.)."""
        current_file = Path(__file__)
        # scripts/cli/engine.py -> ../../ = package root
        package_root = current_file.parents[2]
        
        if not (package_root / "tf").exists():
            raise FileNotFoundError(f"Could not locate package root (tf/ folder missing) at {package_root}")
        return package_root

    def stage_phase(self, phase_name: str) -> Path:
        """
        Stages a specific phase folder (e.g. '0-bootstrap', '1-main').
        Returns path to the staged directory for that phase.
        """
        phase_staging = self.staging_dir / phase_name
        console.print(f"[dim]Staging {phase_name} in {phase_staging}...[/dim]")
        
        if not phase_staging.exists():
            phase_staging.mkdir(parents=True)
            
        # 1. WIPE
        self._wipe_dir(phase_staging)
        
        # 2. COPY PACKAGE TF (from tf/<phase_name>)
        self._copy_phase_tf(phase_name, phase_staging)
        
        # 3. GENERATE BACKEND (unique state file per phase)
        self._generate_backend(phase_name, phase_staging)
        
        # 4. GENERATE TFVARS
        self._generate_tfvars(phase_staging)
        
        # 5. COPY USER FILES (overlay)
        # We copy user files to BOTH phases because we don't know which vars/tf files apply to which phase.
        # Terraform will ignore unused variables (mostly), or user should split them?
        # For simplicity, we copy all user overlay files to both.
        self._copy_user_files(phase_staging)
        
        return phase_staging

    def _wipe_dir(self, target_dir: Path):
        """Removes all *.tf and *.tfvars files in target_dir."""
        for pattern in ["*.tf", "*.tfvars", "*.tfvars.json"]:
            for item in target_dir.glob(pattern):
                item.unlink()
        # Also remove modules folder
        modules_dest = target_dir / "modules"
        if modules_dest.exists():
            shutil.rmtree(modules_dest)

    def _copy_phase_tf(self, phase_name: str, target_dir: Path):
        """Copies TF files from package/tf/<phase_name> to target_dir."""
        phase_source = self.package_root / "tf" / phase_name
        if not phase_source.exists():
            raise FileNotFoundError(f"Phase folder {phase_name} not found in {self.package_root}/tf")
            
        for src in phase_source.glob("*.tf"):
            shutil.copy2(src, target_dir / src.name)
            
        # Copy modules (shared)
        # Assuming modules are at package_root/modules
        src_modules = self.package_root / "modules"
        dest_modules = target_dir / "modules"
        if src_modules.exists():
            shutil.copytree(src_modules, dest_modules)

    def _generate_backend(self, phase_name: str, target_dir: Path):
        """Generates backend.tf with unique state path for the phase."""
        xdg_data = os.environ.get("XDG_DATA_HOME", str(Path.home() / ".local" / "share"))
        state_dir = Path(xdg_data) / "apigee-tf" / "states"
        state_dir.mkdir(parents=True, exist_ok=True)
        
        project_id = self.config.project.gcp_project_id
        suffix = self.config.apigee.state_suffix
        
        # Unique state file per phase!
        # e.g. project-0-bootstrap.tfstate
        base_name = f"{project_id}-{suffix}" if suffix else project_id
        filename = f"{base_name}-{phase_name}"
        
        state_path = state_dir / f"{filename}.tfstate"
        
        backend_tf = f'''# AUTO-GENERATED BY APIM - DO NOT EDIT
terraform {{
  backend "local" {{
    path = "{state_path}"
  }}
}}
'''
        with open(target_dir / "_apim_gen_backend.tf", "w") as f:
            f.write(backend_tf)

    def _generate_tfvars(self, target_dir: Path):
        """Generates auto.tfvars.json in target_dir."""
        tfvars_content = {
            "gcp_project_id": self.config.project.gcp_project_id,
            "apigee_runtime_location": self.config.project.region,
            "apigee_analytics_region": self.config.apigee.analytics_region,
            "domain_name": self.config.network.domain,
            "default_root_domain": self.config.network.default_root_domain,
            "control_plane_location": self.config.apigee.control_plane_location,
            "billing_type": self.config.apigee.billing_type, # Fixed var name mismatch? check variables.tf
            "apigee_billing_type": self.config.apigee.billing_type, # Providing both for safety if I forgot to rename one
            "apigee_instance_name": self.config.apigee.instance_name,
        }
        
        if self.config.apigee.consumer_data_region:
            tfvars_content["consumer_data_region"] = self.config.apigee.consumer_data_region
        
        tfvars_path = target_dir / "_apim_gen.auto.tfvars.json"
        with open(tfvars_path, "w") as f:
            json.dump(tfvars_content, f, indent=2)

    def _copy_user_files(self, target_dir: Path):
        """Copies user's overlay *.tf and *.tfvars files to target_dir."""
        reserved_prefixes = ["_apim_"]
        count = 0
        for pattern in ["*.tf", "*.tfvars", "*.tfvars.json"]:
            for source_file in self.project_root.glob(pattern):
                for prefix in reserved_prefixes:
                    if source_file.name.startswith(prefix):
                        continue # Skip reserved
                
                dest = target_dir / source_file.name
                shutil.copy2(source_file, dest)
                count += 1
